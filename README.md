# ğŸ¬ PLN Movies

RepositÃ³rio criado para a disciplina de **Processamento de Linguagem Natural (PLN)**.

O objetivo deste projeto Ã© **coletar, estruturare trabalhar em cima de informaÃ§Ãµes sobre filmes** a partir do site [JustWatch](https://www.justwatch.com/br/filmes).

## Como funciona

O fluxo atual Ã© composto por **duas etapas principais de coleta**, responsÃ¡veis por extrair e estruturar os dados necessÃ¡rios para anÃ¡lise.
Depois disso, ...

---

### 1. Coleta dos links de filmes e de suas informaÃ§Ãµes

**Arquivo:** `get_movies_links.py`

Este script realiza a navegaÃ§Ã£o automÃ¡tica na pÃ¡gina principal do JustWatch e extrai os links de todos os filmes exibidos.

* Utiliza **Selenium** para rolar a pÃ¡gina inicial dinamicamente, garantindo que "todos" os tÃ­tulos sejam carregados, considerando sua limitaÃ§Ã£o de scrolls.
* Localiza os elementos HTML correspondentes aos links dos filmes.
* Gera o arquivo `data/movies_links.csv`, contendo os links individuais de cada tÃ­tulo encontrado â€” este arquivo Ã© **essencial para a prÃ³xima etapa** da coleta.

**Arquivo:** `get_movies_info.py`

Com base nos links coletados anteriormente, este script acessa cada pÃ¡gina individual de filme e extrai suas **informaÃ§Ãµes detalhadas**.

* Utiliza **Requests** e **BeautifulSoup** para acessar e interpretar o conteÃºdo HTML de cada pÃ¡gina.
* Extrai informaÃ§Ãµes como:
  * TÃ­tulo 
  * Ano de lanÃ§amento
  * Plataformas de streaming disponÃ­veis
  * Sinopse original -> **lemmatizada** e **com stemming**
  * AvaliaÃ§Ãµes (JustWatch, Rotten Tomatoes, IMDb)
  * GÃªneros
  * DuraÃ§Ã£o do filme
  * ClassificaÃ§Ã£o indicativa

* Realiza o prÃ©-processamento textual utilizando **NLTK**, aplicando *tokenizaÃ§Ã£o*, *remoÃ§Ã£o de stopwords*, *lemmatizaÃ§Ã£o* e *stemming*.
* Salva todas as informaÃ§Ãµes no arquivo `data/movies_info.csv`, gerando o dataset consolidado.

> IMPORTANTE: Antes da primeira execuÃ§Ã£o, Ã© necessÃ¡rio baixar os recursos do NLTK (comentados no inÃ­cio do script). ApÃ³s a primeira execuÃ§Ã£o, essas linhas podem ser comentadas novamente.

### ğŸï¸ 2. Coleta das informaÃ§Ãµes dos filmes

*(em desenvolvimento)*

---

## ğŸ“‚ Estrutura dos arquivos gerados

| Arquivo                | DescriÃ§Ã£o                                                         |
| ---------------------- | ----------------------------------------------------------------- |
| **`movies_links.csv`** | ContÃ©m os links individuais de cada filme coletado no JustWatch.  |
| **`movies_info.csv`**  | Dataset com os metadados e informaÃ§Ãµes detalhadas de cada tÃ­tulo. |

---

Perfeito, Nicole! Mantendo o mesmo padrÃ£o do README e atualizando com o nome correto do arquivo (`analyze_movies.py`), a seÃ§Ã£o de vetorizaÃ§Ã£o e recomendaÃ§Ã£o ficaria assim:

---

### 2. VetorizaÃ§Ã£o, anÃ¡lise e sistema de recomendaÃ§Ã£o

**Arquivo:** `analyze_movies.py`

Este script realiza o **pipeline completo de anÃ¡lise** a partir do dataset consolidado `movies_info.csv`:

* **Carrega os dados** previamente coletados (`movies_info.csv`).
* **PrÃ©-processa os textos** (sinopses lemmatizadas), mantendo o pipeline pronto para anÃ¡lises posteriores.
* **Cria representaÃ§Ãµes vetoriais** dos textos de filmes utilizando diferentes abordagens:
  * **Bag-of-Words (BoW)**
  * **TF-IDF**
  * **Sentence-BERT embeddings** (para similaridade semÃ¢ntica entre sinopses)

* **Executa o sistema de recomendaÃ§Ã£o**:
  * Agrupa filmes em **clusters** com base na similaridade de conteÃºdo
  * Cria **projeÃ§Ãµes PCA** para visualizaÃ§Ã£o
  * Permite recomendar filmes similares por **tÃ­tulo** ou por **query textual**

* **Salva os resultados**:
  * Vetores de representaÃ§Ã£o (BoW, TF-IDF e SBERT) na pasta `data/vectorized/`
  * Resultados de clustering e projeÃ§Ãµes PCA na mesma pasta

**Exemplos de uso:**
* Recomendar filmes similares a um tÃ­tulo especÃ­fico e por uma consuta textual:

```python
recommend_movies("Superman(2025)", recommender)
search_movies("HerÃ³is", recommender)
```

> IMPORTANTE: Os mÃ³dulos `Vectorizer` e `RecommendationSystem` contÃªm a lÃ³gica de vetorizaÃ§Ã£o e recomendaÃ§Ã£o. O prÃ©-processamento textual completo, incluindo tokenizaÃ§Ã£o, lemmatizaÃ§Ã£o e stemming, pode ser integrado usando `TextPreprocessor` (atualmente comentado no script).

---

Perfeito! Esse `vetorize.py` Ã© **o mÃ³dulo de vetorizaÃ§Ã£o** usado pelo seu `analyze_movies.py`.

Em termos de documentaÃ§Ã£o, podemos descrever assim:

---

### 3. VetorizaÃ§Ã£o de textos e cÃ¡lculo de similaridade

**Arquivo:** `vetorize.py`

Este mÃ³dulo fornece uma classe `Vectorizer` para transformar as sinopses de filmes em **representaÃ§Ãµes vetoriais**, essenciais para anÃ¡lise e recomendaÃ§Ã£o:

* **BoW (Bag-of-Words)**: cria uma matriz de frequÃªncia de palavras, ignorando palavras muito raras ou muito frequentes.
* **TF-IDF**: cria uma matriz ponderada de termos, considerando sua importÃ¢ncia relativa no corpus.
* **SBERT (Sentence-BERT)**: gera embeddings semÃ¢nticos das sinopses para cÃ¡lculo de similaridade mais avanÃ§ado.

**Funcionalidades principais:**

* `create_bow_vectors(corpus)`: gera a matriz BoW e retorna os recursos (palavras).
* `create_tfidf_vectors(corpus)`: gera a matriz TF-IDF e retorna os recursos.
* `create_sbert_embeddings(corpus)`: gera embeddings SBERT normalizados para cada documento.
* `calculate_similarity_matrix(method="sbert")`: calcula matriz de similaridade entre todos os documentos (`bow`, `tfidf` ou `sbert`).
* `search_similar_documents(query, top_k=5)`: retorna os Ã­ndices e similaridades dos documentos mais prÃ³ximos a uma query.
* `save_vectors(output_dir)`: salva matrizes BoW, TF-IDF e embeddings SBERT em arquivos CSV e `.npy` na pasta especificada.

* Gera arquivos de vetores na pasta `data/vectorized/`:

| Arquivo                | DescriÃ§Ã£o                                                                              |
| ---------------------- | -------------------------------------------------------------------------------------- |
| `bow_matrix.csv`       | Matriz **Bag-of-Words** das sinopses, representando a frequÃªncia de palavras.          |
| `tfidf_matrix.csv`     | Matriz **TF-IDF** das sinopses, com a importÃ¢ncia relativa de cada termo.       |
| `sbert_embeddings.npy` | Embeddings **SBERT** normalizados das sinopses para cÃ¡lculo de similaridade semÃ¢ntica. |
| `sbert_embeddings.csv` | VersÃ£o em CSV dos embeddings SBERT             |


> IMPORTANTE: SBERT requer download do modelo `"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"` na primeira execuÃ§Ã£o, o que pode levar alguns minutos dependendo da conexÃ£o.

---

## DependÃªncias

Selenium â€“ automaÃ§Ã£o da navegaÃ§Ã£o web.
BeautifulSoup â€“ extraÃ§Ã£o de informaÃ§Ãµes do HTML.
pandas â€“ manipulaÃ§Ã£o e estruturaÃ§Ã£o de dados.
requests â€“ requisiÃ§Ãµes HTTP.
nltk â€“ prÃ©-processamento de texto (lemmatizaÃ§Ã£o, stemming, tokenizaÃ§Ã£o).
scikit-learn â€“ ferramentas de machine learning e prÃ©-processamento.
scipy â€“ funÃ§Ãµes cientÃ­ficas e estatÃ­sticas.
numpy â€“ operaÃ§Ãµes numÃ©ricas e matrizes.
sentence-transformers â€“ embeddings de sentenÃ§as e similaridade semÃ¢ntica.
matplotlib â€“ visualizaÃ§Ã£o de dados.
seaborn â€“ visualizaÃ§Ã£o estatÃ­stica de dados.

